---
title: "iZettle Business Use Case"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)


library(RMySQL)   #MySQL interaction
library(dplyr)    #data handling
library(caret)    #classification and regression training
#library(ggplot2)   #for graphs
library(magrittr)  #for pipes
library(parallel)  #parallel processing for model training
library(doParallel) #ditto

#SQL connection:
mydb <- dbConnect(MySQL(), dbname = "ToyStorey", host = 'db.cfda.se', 
                port = 3306, user = "toystorey", password = "toys@sse") 
```

```{r fullCOde, echo = F, eval = T, warning = F, message = F}

### LOAD DATA:

#establish MySQL connection:
sql_con = dbConnect(MySQL(), dbname = "IZettle", host = "db.cfda.se", 
                port = 3306, user = "toystorey", password = "toys@sse") 
#fetch entire Order table
query <- 'SELECT * FROM IZettleOrder;'
data <- fetch(dbSendQuery(sql_con,query), n=-1)
#df <- data #make copy
#data <- df #restore

# Load Customer aggregates (like "Group by") - for all 673813 customers!
customers <- data %>% 
  group_by(cid) %>%  
  summarise(NumberPurchases = n(), TotalSales = sum(as.numeric(purchase_amount)), AvgSales = mean(as.numeric(purchase_amount)),
            #Genders
            nGender = n_distinct(gender), Male = sum(gender == "male"), Female = sum(gender == "female"),
            #Birthyears - paste together the different values for one customer
            nBirthyear = n_distinct(birthyear), Birth = toString(unique(birthyear)),
            #At least one mobile purchase:
            nonDesktopPurchases = sum(target == "0", na.rm = T))

#Clean data: 
#fix genders (1836 have multiple, AKA "none" plus the real gender):
customers$trueGender <- case_when(
  customers$Male > 0 ~ "male",
  customers$Female > 0 ~ "female",
  TRUE ~ "none"
)

#fix Birthyears (204 have multiple)
customers$trueBirthyear <- stringr::str_extract(customers$Birth, "[[:digit:]]+")

#Join Customer characteristics onto main dataset:
data <- left_join(data, customers[,c("cid","trueBirthyear", "trueGender", "NumberPurchases","TotalSales","AvgSales","nonDesktopPurchases")], by = "cid")

#We can see that top buyers still do not have genders or birthyears...
#View(arrange(customers, desc(TotalSales)))


#Create target: customers with at least one mobile purchase (mobile users), have multiple purchases (not churn), and total purchases below some sum (not B2B)
mult_customers <- filter(customers, NumberPurchases > 1) %>% select(cid)   #customer IDs who have bought multiple
data$target_customer <- ifelse(data$nonDesktopPurchases > 0 & data$cid %in% mult_customers$cid & data$TotalSales < 1500000, 1, 0)


#filter out missing targets
data2 <- filter(data, !is.na(target) & !is.na(trueBirthyear) & trueGender!= "none") 

#keep only first obsevation 
data2$datestamp %<>% as.Date()  #turn into date
df_final <- data2 %>%
  group_by(cid) %>%
  arrange(datestamp) %>%
  slice(1L) #keep first row in grouped,arranged dataset

### Create Target/ Features
#make sure target variable is factor (for classification models; for regression models, not needed):
df_final$target_customer %<>% as.factor()
levels(df_final$target_customer) <- c("nontarget","target")


#Fix NAs (check NAS)
#sapply(df_final, function(x) sum(is.na(x)))
df_final$trueBirthyear %<>% as.numeric()
#df_final[is.na(df_final)] <- 0


#change to proper variables:
factorcols <- c("id","cid","currency","birthyear","gender","merchant_id","country","device","target","trueGender","trueBirthyear")
df_final[factorcols] <- lapply(df_final[factorcols], factor) 
df_final$purchase_amount %<>% as.numeric()
df_final$NumberPurchases %<>% as.numeric()
df_final$TotalSales %<>% as.numeric()
df_final$AvgSales %<>% as.numeric()
df_final$month <- lubridate::month(df_final$datestamp)
#df_final$trueBirthyear %<>% as.numeric() %>% as.factor()

df_final$sweden <- as.factor(ifelse(df_final$country == "se", 1, 0))
df_final$norway <- as.factor(ifelse(df_final$country == "no", 1, 0))


#remove some vars (TODO) just to be certain...: (model training wassn't working!!)
df_final$datestamp <- NULL
df_final$currency <- NULL
df_final$device <- NULL
df_final$target <- NULL
df_final$gender <- NULL
df_final$birthyear <- NULL
df_final$country <- NULL
df_final$nonDesktopPurchases <- NULL

#Write CSVs for IBM machine learning:
#write.csv(df_final, "IBMizettleFULL.csv")
#smallSampleIndex <- createDataPartition(df_final$target_customer, p = 0.05, list = F)
#smallSample <- df_final[smallSampleIndex,]
#write.csv(smallSample, "IBMizettleSMALL.csv")


### Split into Train/Test

#partition the data evenly along the dependent variable: 70% will be training, 30% testing
training.indices <- createDataPartition(df_final$target_customer, p = 0.5, list = F)
df_final2 <- df_final[training.indices,]
#df_final2 <- df_final

training.indices <- createDataPartition(df_final2$target_customer, p = 0.7, list = F)
#training data set
df_train <- df_final2[training.indices,]
#testing data set
df_test <- df_final2[-training.indices,]


### Different Models:


#train a LDA model with caret:
lda.model <- caret::train(
  form = target_customer ~ purchase_amount + merchant_id + trueBirthyear + trueGender + AvgSales + month + sweden + norway + TotalSales +NumberPurchases ,
  data = df_train,
  trControl = trainControl(method = "cv", number = 5, classProbs = T, summaryFunction = twoClassSummary),
  method = "lda"
)   #81% accuracy, low specificity
confusionMatrix(predict(lda.model, df_train), df_train$target_customer, positive = "target")
confusionMatrix(predict(lda.model, df_test), df_test$target_customer, positive = "target")

#train a QDA model with caret:
qda.model <- caret::train(
  form = target_customer ~ purchase_amount + merchant_id + trueBirthyear + trueGender + AvgSales + month + sweden + norway + TotalSales +NumberPurchases ,
  data = df_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "qda"
)   #75% accuracy, 20%specificity, not better than NIR though
confusionMatrix(predict(qda.model, df_train), df_train$target_customer, positive = "target")
confusionMatrix(predict(qda.model, df_test), df_test$target_customer, positive = "target")


#train a Random Forest model with caret:
smallIndex <- createDataPartition(df_train$target_customer, p = 0.01, list = F)
smalltrain <- df_train[smallIndex,]
rf.model = caret::train(
  form = target_customer ~  purchase_amount + merchant_id + trueBirthyear + trueGender + AvgSales + month + sweden + norway + TotalSales +NumberPurchases ,
  data = smalltrain,
  trControl = trainControl(method = "cv", number = 5),
  method = "rf"
)  #90% accuracy, 77% specificity, better than NIR
rf.cm.train <- confusionMatrix(predict(rf.model, df_train), df_train$target_customer, positive = "target")
rf.cm.test <- confusionMatrix(predict(rf.model, df_test), df_test$target_customer, positive = "target")


##XGBoosted Tree : 
#https://analyticsdataexploration.com/xgboost-model-tuning-in-crossvalidation-using-caret-in-r/

parametersGrid <-  expand.grid(eta = 0.1, 
                            colsample_bytree=c(0.5,0.7),
                            max_depth=c(3,6),
                            nrounds=100,
                            subsample =  c(0.5, 1),
                            gamma=1,
                            min_child_weight=2
                            )

modelxgboost <- caret::train(
  form = target_customer ~  purchase_amount + merchant_id + trueBirthyear + trueGender + AvgSales + month + sweden + norway + NumSales,
  data = smalltrain,
  trControl = trainControl(method = "cv", number = 5, savePredictions = T, classProbs = T),
  method = "xgbTree",
  tuneGrid = parametersGrid
) 

xgb.cm.train <- confusionMatrix(predict(modelxgboost, df_train), df_train$target_customer, positive = "target")
xgb.cm.test <- confusionMatrix(predict(modelxgboost, df_test), df_test$target_customer, positive = "target")

# Look at variable importance:
varimp <- varImp(rf.model)
z <- varimp$importance
z$percent <- as.numeric(z$Overall / sum(z$Overall))



##Correlation matrix:
#fit <- aov(target_customer ~., data=correlated)

```




\newpage

#Summary:

**Business Use Case**: Predict whether a customer will churn (i.e. whether a customer has only made one purchase). 

**Target**: *is_churn*: "1 if customer did NOT return, 0 if customer did return"

**Features**: as defined in "Exploratory Data Analysis" Lab. 

- *home_delivery*: Binary variable (1 if home delivery, 0 if collected in store)
- *num_items_sold*: The number of items included in the order
- *sales*: The total purchase amount in SEK
- *shipping_cost*: How much the customer paid for shipping in SEK
- *discount*: If a discount was applied to order

Furthermore, **Principal Component Analysis (PCA)** was applied to the features in an attempt to root out unhelpful correlations between the features and find a lower dimensional representation of the data (and hopefully feature selection). However, the PCA applied below led to very limited variable selection: only the very last two principle components were cut, as the share of variation in the data explained by every subsequent PC incorporated was roughly even (as seen in the linear cumulative explained variance plot below); thus it was fruitless to narrow down variables much more than that. (We keep the first 55 principal components, as the last two PCs explain relatively smaller amounts of variance - about a third as much)



\newpage


#Data Preparation: 

##Load datasets:

```{r}

set.seed(7313)
#partition the data evenly along the dependent variable: 70% will be training, 30% testing
training.indices <- createDataPartition(df$is_churn, p = 0.7, list = F)
#training data set
churn_train <- df[training.indices,]
#testing data set
churn_test <- df[-training.indices,]

```


##Create a "train control" object

This train control object will apply to all models below. We use 5-fold cross validation. We choose "twoClassSummary" so that we can access metrics other than Accuracy.

```{r}

control <- trainControl(method = "cv", number = 5, allowParallel = T,
                       classProbs = T, summaryFunction = twoClassSummary)

```

##Dimension Reduction (PCA):

We do [PCA transformation](https://en.wikipedia.org/wiki/Principal_component_analysis) to generate principal component features, which are uncorrelated (orthogonal) linear combinations of our raw features that capture the highest variance in the (remaining) data: 


```{r}

#Data Preparation:
#remove the target (and other vars?)
x.train <- churn_train %>% dplyr::select(-customer_id, -is_churn)
x.test <- churn_test %>% dplyr::select(-customer_id, -is_churn)

#PCA on training set (minus target!)
pca_train <- prcomp(x.train, scale = T, center = T)
#calculate variance
pca_train$var <- pca_train$sdev^2
#how much variance explained by each component
pve <- pca_train$var/sum(pca_train$var)
#pve   
#graph of cumulative sum of variance explained by each PC
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cum. Prop. of Var. Explained")


```

Based on the amount of variance we see explained by the subsequent Principal Components, we'll cap how many PCs we use. It appears, though, that the cumulative proportion of variance explained by every additional component is rather linear, meaning that we can't reduce the number of variables significantly without losing variance in our data. Still, we'll cap it at 55 principal components (out of 59 original features) because the last few PCs appear to explain less variation than the first 55. (just for fun)

We do PCA transformation using "preProcess" from the caret package here (for consistency), creating the principal components out of the x variables, then adding by the y variable (churn):

```{r}

#Data Preparation:
#remove the target 
#x.train <- churn_train %>% dplyr::select(-customer_id, -is_churn)
#x.test <- churn_test %>% dplyr::select(-customer_id, -is_churn)

#create a pre-processing transformation object
preProc <- preProcess(x.train, #don't include target
                      method = c("pca", "scale", "center"),   
                      pcaComp = 55)  #how many PCA components to keep (up to "p")

#apply (same) preprocessing to both train and test sets (minus target):
#Thus, we have our (x) features (all PC's or combinations of previous features)
train.pca <- predict(preProc, x.train)
test.pca <- predict(preProc, x.test)
#add the target variable (y) back:
train.pca$is_churn <- churn_train$is_churn
test.pca$is_churn <- churn_test$is_churn


```




#Models (all features):

See a list of models available in R package for classification and regression training *caret* (and their tuning parameters) [here](https://topepo.github.io/caret/available-models.html):

Various methods of estimating model accuracy with caret [here](https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/). (Note in the following models we just use the same train control objects).

Tuning the "trainControl" object [here](https://rdrr.io/cran/caret/man/trainControl.html).


##Logistic Model

##LDA model:

##QDA model:

##Random Forest:



#Comparison of Models

Using the models, we can see that QDA (both using all variables and using PCA variables) generally scores high on sensitivity (the true positive rate, TPR), while Logistic and LDA models score high on Specificity (True Negative Rate, TNR). However, Random forest, seems to do best on both metrics, and therefore does best with ROC. Furthermore, we can see RF does best for accuracy as well. Thus, we choose our **Random Forest** model.

Thus, we choose the **Random Forest** (using PCA with *some* dimension reduction) as our best model.

#Estimated value impact

Finally, we can evaluate the impact of our model, making a few (unrealistic) simplifying assumptions:

- The cost of our efforts is 0 SEK (reaching out to churners, bidding them to return to the store, etc)
- Our efforts are 100% successful

We can see the results of our chosen model (Random Forest), when applied to test data:

As we can see, when our model predicts "churn", it is correct ~70% of the time (the *Positive Pred Value*, or Correct # churn / Total pred churn). So out of 1000 new customers that are predicted as "churn", ~700 can be expected to actually churn.

From the database we can calculate the sum of sales and the average sales per customer (of ALL sales, not just the first stale), grouped by those customers that churn vs those that are loyal.

Therefore, for every customer we can retain, we expect on average to make **212 SEK** more (the average loyal customer has average sales of 255 SEK while the average churner has average sales of 43 SEK). Therefore, if retention costs are below 212 SEK per customer retained, we can expect a positive net result. For the following value impact, though, **we'll assume it costs nothing to retain a customer, and retaining an identified churner is 100% certain** (not realistic, adjust later):

One very basic measure of total expected value, assuming we reach out to EVERYONE we suspect as a churner, then, could be calculated by taking the number of correctly predicted churners (~50,000) and multiply by the difference in average sales (between churners and loyal customers), or 212 SEK. We get ~50,000\*212 = 10.6 million SEK.

A more marginal (and thus applicable) estimate of value impact: For every 1000 predicted churners that we approach (and successfuly retain at no cost), we can assume 700 are actually would-be churners, which, if we retain, will net us 700*212 SEK, or 148,400 SEK.

To be more realistic, then, we can multiply our estimate by a probability of success (we retain X% of customers we approach, e.g.) and also subtracting out costs for the effort of reaching predicted churners. To be done later...



